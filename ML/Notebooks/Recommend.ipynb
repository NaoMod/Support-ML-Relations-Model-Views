{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "Note that some packages are local python scripts created to enable the proper use of the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic configurations\n",
    "\n",
    "Define the main options for the Training and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from os import path as osp\n",
    "import json\n",
    "\n",
    "#CONFIG EXPERIMENT\n",
    "SPLIT = \"80/10/10\"\n",
    "BATCH = \"_batch\"\n",
    "ADD_NEGATIVE_TRAINING = False\n",
    "F1_TR = 0.5\n",
    "\n",
    "# important directories\n",
    "DATASET_LABEL = \"AB_new\"\n",
    "\n",
    "data_path = osp.join('../data/', DATASET_LABEL)\n",
    "trained_path = osp.join('../trained/', DATASET_LABEL)\n",
    "save_path_csv = osp.join('../results/', DATASET_LABEL)\n",
    "\n",
    "save_path_csv = osp.join(save_path_csv, 'results' + BATCH + '.csv')\n",
    "\n",
    "raw_path = osp.join(data_path, 'raw')\n",
    "\n",
    "# Read JSON to get extra parameters in the graph\n",
    "json_path = glob.glob(osp.join(raw_path, \"Parameters.json\"))\n",
    "parameters_exist = len(json_path) != 0\n",
    "parameters_for_gnn = None\n",
    "if parameters_exist:\n",
    "    parameters_for_gnn = json_path[0]\n",
    "\n",
    "with open(parameters_for_gnn) as json_data:\n",
    "    parameters = json.load(json_data)\n",
    "\n",
    "class_left = parameters['CLASS_LEFT']\n",
    "class_right = parameters['CLASS_RIGHT']\n",
    "relation_name = parameters['RELATION_NAME']\n",
    "rev_relation_name = \"rev_\" + relation_name\n",
    "EPOCHS = parameters['EPOCHS']\n",
    "LEARNING_RATE = parameters['LEARNING_RATE']\n",
    "\n",
    "UNIQUE_ID_LEFT = parameters['UNIQUE_ID_LEFT']\n",
    "UNIQUE_ID_RIGHT = parameters['UNIQUE_ID_RIGHT']\n",
    "\n",
    "IGNORE_ATTRIBUTES_LEFT = parameters['IGNORE_ATTRIBUTES_LEFT']\n",
    "IGNORE_ATTRIBUTES_RIGHT = parameters['IGNORE_ATTRIBUTES_RIGHT']\n",
    "\n",
    "STRINGS_ENCODER = parameters['STRINGS_ENCODER']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print functions for the performance curves\n",
    "\n",
    "Functions to print both ROC (Receiver Operating Characteristic) curve and PR (Precision-Recall) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pr_curve(precision, recall, title, pr_auc, no_skill):\n",
    "    #create precision recall curve\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, color='purple', label='AUC = %0.4f' % pr_auc)\n",
    "    ax.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "\n",
    "    #add axis labels to plot\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_xlabel('Recall')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    #display plot\n",
    "    plt.show\n",
    "\n",
    "def print_roc_curve(fpr, tpr, title, roc_auc):\n",
    "    #create ROC curve\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, color='purple', label='AUC = %0.4f' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
    "\n",
    "    #add axis labels to plot\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('True Positive Rate(TPR)')\n",
    "    ax.set_xlabel('False Positive Rate(FPR)')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    #display plot\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define directories to get the data and to save the results of the proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folders = {\n",
    "    \"rule_1\" : \"A.a = B.b\",\n",
    "    # \"rule_2\" : \"A.a = B.b + B.c\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of EMF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecore.resources import ResourceSet, URI\n",
    "\n",
    "# Custom function get Heterodata from EMF model\n",
    "from utils.ab import Func\n",
    "\n",
    "# Register the metamodels in the resource set    \n",
    "resource_set = ResourceSet()\n",
    "ecore_path_a = glob.glob(osp.join(data_path, 'metamodels/A.ecore'))[0] # only one metamodel\n",
    "ecore_path_b = glob.glob(osp.join(data_path, 'metamodels/B.ecore'))[0] # only one metamodel   \n",
    "resource_a = resource_set.get_resource(URI(ecore_path_a))\n",
    "mm_root_a = resource_a.contents[0]\n",
    "resource_b = resource_set.get_resource(URI(ecore_path_b))\n",
    "mm_root_b = resource_b.contents[0]\n",
    "\n",
    "resource_set.metamodel_registry[mm_root_a.nsURI] = mm_root_a\n",
    "resource_set.metamodel_registry[mm_root_b.nsURI] = mm_root_b\n",
    "\n",
    "# TODO: Consider the ignore attributes from JSON\n",
    "dataset_func = Func(sentence_encoding_name=STRINGS_ENCODER, features_to_encode_left=['a', 's'], features_to_encode_right=['b', 'c', 'd','s'], unique_id_left=UNIQUE_ID_LEFT, unique_id_right=UNIQUE_ID_RIGHT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Hetero data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_rule_name = \"rule_1\"\n",
    "# Register the models in the resource set    \n",
    "xmi_path_left = glob.glob(osp.join(raw_path, directory_rule_name, \"DatasetLeft.xmi\"))[0]\n",
    "m_resource_left = resource_set.get_resource(URI(xmi_path_left))\n",
    "model_root_left = m_resource_left.contents\n",
    "\n",
    "xmi_path_right = glob.glob(osp.join(raw_path, directory_rule_name, \"DatasetRight.xmi\"))[0]\n",
    "m_resource_right = resource_set.get_resource(URI(xmi_path_right))\n",
    "model_root_right = m_resource_right.contents\n",
    "\n",
    "relations_path = glob.glob(osp.join(raw_path, directory_rule_name, \"Relations.csv\"))\n",
    "relations_exist = len(relations_path) != 0\n",
    "relations_for_graph = None\n",
    "if relations_exist:\n",
    "    relations_for_graph = relations_path[0]   \n",
    "\n",
    "data, left_original_mapping, right_original_mapping = dataset_func.xmi_to_graph(model_root_left, model_root_right, relations_for_graph, parameters_for_gnn)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add reverse relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    " # Add a reverse (class_right, rev_relation_name, class_left) relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data[class_right, rev_relation_name, class_left].edge_label  # Remove \"reverse\" label."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1, # 10% of links for validation\n",
    "    num_test=0.1, # 10% os links for test\n",
    "    disjoint_train_ratio=0.3, # use 70% for message passing and 30% for supervision\n",
    "    neg_sampling_ratio=1.0, # generate 2:1 negative edges\n",
    "    add_negative_train_samples=ADD_NEGATIVE_TRAINING,\n",
    "    edge_types=(class_left, relation_name, class_right),\n",
    "    rev_edge_types=(class_right, rev_relation_name, class_left)\n",
    ")(data)\n",
    "\n",
    "edge_label_index = train_data[class_left, relation_name, class_right].edge_label_index\n",
    "edge_label = train_data[class_left, relation_name, class_right].edge_label\n",
    "\n",
    "print(train_data)\n",
    "print(edge_label_index)\n",
    "print(edge_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADERS\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio=1.0,\n",
    "    edge_label_index=((class_left, relation_name, class_right), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN architecture\n",
    "\n",
    "Two SageConv Layers for Node embedding and a Classifier that performs the dot-product between embeddings. It also use a extra `to_hetero` transformer since it will be applied to the HeteroData objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This class is responsible for creating the ML model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method is responsible for executing the forward pass of the convolutional GNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            input tensor\n",
    "        edge_index: torch.Tensor\n",
    "            edge index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: torch.Tensor\n",
    "            output tensor\n",
    "        \"\"\"\n",
    "        # A 2-layer GNN computation graph.\n",
    "        # `ReLU` is the non-lineary function used in-between.\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The final classifier applies the dot-product between source and destination node embeddings to derive edge-level predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor, edge_label_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method is responsible for executing the forward pass of the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_left: torch.Tensor\n",
    "            input tensor for Left class\n",
    "        x_right: torch.Tensor\n",
    "            input tensor for Right class\n",
    "        edge_label_index: torch.Tensor\n",
    "            edge label index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: torch.Tensor\n",
    "            output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_left  = x_left[edge_label_index[0]]\n",
    "        edge_feat_right = x_right[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_left * edge_feat_right).sum(dim=-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This class is responsible for creating the model with two SageConv layers with a addiotinal classifier to be used in the training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_channels, data):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for A and B:\n",
    "        self.left_lin = torch.nn.Linear(data[class_left].num_features, hidden_channels)\n",
    "        self.right_lin = torch.nn.Linear(data[class_right].num_features, hidden_channels)\n",
    "        # self.left_emb = torch.nn.Embedding(data[class_left].num_nodes, hidden_channels)\n",
    "        # self.right_emb = torch.nn.Embedding(data[class_right].num_nodes, hidden_channels)\n",
    "\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        \"\"\"\t\n",
    "        This method is responsible for executing the forward pass of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: HeteroData\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred: torch.Tensor\n",
    "            output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        x_dict = {\n",
    "        #   class_left: self.A_lin(data[class_left].x) + self.left_emb(data[class_left].node_id),\n",
    "          class_left: self.left_lin(data[class_left].x) ,#+ self.left_emb(data[class_left].node_id),\n",
    "          class_right: self.right_lin(data[class_right].x) ,#+ self.right_emb(data[class_right].node_id),\n",
    "        } \n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        pred = self.classifier(\n",
    "            x_dict[class_left],\n",
    "            x_dict[class_right],\n",
    "            data[class_left, relation_name, class_right].edge_label_index,\n",
    "        )\n",
    "        return pred\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"dot_product\"\n",
    "    \n",
    "model = Model(hidden_channels=64, data=data)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Heterogeneous Link-level GNN\n",
    "\n",
    "Training the GNN is similar to training any PyTorch model.\n",
    "We move the model to the desired device, and initialize an optimizer that takes care of adjusting model parameters via stochastic gradient descent.\n",
    "\n",
    "The training loop then iterates over the training mini-batches, applies the forward computation of the model, computes the loss from ground-truth labels and obtained predictions using binary cross entropy, and adjusts model parameters via back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(LEARNING_RATE))\n",
    "\n",
    "for epoch in range(1, int(EPOCHS)):\n",
    "    total_loss = total_examples = 0\n",
    "    for _, sampled_data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sampled_data.to(device)\n",
    "        pred = model(sampled_data)\n",
    "\n",
    "        ground_truth = sampled_data[class_left, relation_name, class_right].edge_label\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.numel()\n",
    "        total_examples += pred.numel()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Heterogeneous Link-level GNN\n",
    "\n",
    "After training, we evaluate our model on useen data coming from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation seed edges:\n",
    "edge_label_index = val_data[class_left, relation_name, class_right].edge_label_index\n",
    "edge_label = val_data[class_left, relation_name, class_right].edge_label\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=(\n",
    "        (class_left, relation_name, class_right), \n",
    "        val_data[(class_left, relation_name, class_right)].edge_label_index,\n",
    "    ),\n",
    "    edge_label=val_data[(class_left, relation_name, class_right)].edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(val_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)\n",
    "\n",
    "assert sampled_data[class_left, relation_name, class_right].edge_label_index.size(1) == 3 * 128\n",
    "assert sampled_data[class_left, relation_name, class_right].edge_label.min() >= 0\n",
    "assert sampled_data[class_left, relation_name, class_right].edge_label.max() <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc as area_curve\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm.tqdm(val_loader):\n",
    "    with torch.no_grad():\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[class_left, relation_name, class_right].edge_label)\n",
    "\n",
    "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "print(\"Sample of y_true and y_score\")\n",
    "print(ground_truth[:10])\n",
    "print(pred[:10])\n",
    "\n",
    "auc = roc_auc_score(ground_truth, pred)\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "\n",
    "fpr, tpr, roc_tr = roc_curve(ground_truth, pred)\n",
    "print_roc_curve(fpr, tpr, DATASET_LABEL + \"_Validation\", auc)\n",
    "\n",
    "# no_skill = len(ground_truth[ground_truth==1]) / len(ground_truth)\n",
    "# precision, recall, _ = precision_recall_curve(ground_truth, (torch.sigmoid(pred)))\n",
    "# pr_auc = area_curve (recall, precision)\n",
    "#print_pr_curve(precision, recall, DATASET_LABEL + \"_Validation\", pr_auc)\n",
    "\n",
    "#f1 = f1_score(ground_truth, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "roc_auc = area_curve(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n",
    "\n",
    "####################################\n",
    "# The optimal cut off would be where tpr is high and fpr is low\n",
    "# tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n",
    "####################################\n",
    "i = np.arange(len(tpr)) # index for df\n",
    "roc = pd.DataFrame({\n",
    "    'fpr' : pd.Series(fpr, index=i),\n",
    "    'tpr' : pd.Series(tpr, index = i), \n",
    "    '1-fpr' : pd.Series(1-fpr, index = i), \n",
    "    'tf' : pd.Series(tpr - (1-fpr), index = i), \n",
    "    'thresholds' : pd.Series(roc_tr, index = i)})\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'])\n",
    "plt.plot(roc['1-fpr'], color = 'red')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "ax.set_xticklabels([])\n",
    "plt.show()\n",
    "\n",
    "print(roc.iloc[(roc.tf-0).abs().argsort()[:1]]['thresholds'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict results to be used in the new view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = model.cpu() \n",
    "model.eval() \n",
    "\n",
    "total_class_left = len(left_original_mapping) \n",
    "total_class_right = len(right_original_mapping)\n",
    "\n",
    "threshold = 0.7\n",
    "predictions = {}\n",
    "\n",
    "for uri_idenifier_left, left_id in tqdm(left_original_mapping.items()):\n",
    "    predictions[uri_idenifier_left] = []\n",
    "\n",
    "    left_row = torch.tensor([left_id] * total_class_right) \n",
    "    all_right_ids = torch.arange(total_class_right) \n",
    "    edge_label_index = torch.stack([left_row, all_right_ids], dim=0) \n",
    "    data[class_left, relation_name, class_right].edge_label_index = edge_label_index\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        pred = model(data) \n",
    "    probabilities = torch.sigmoid(pred)\n",
    "    pred_labels = (probabilities > threshold).long()\n",
    "\n",
    "    recommended_links = all_right_ids[pred_labels == 1].tolist()\n",
    "\n",
    "    predictions[uri_idenifier_left] = recommended_links\n",
    "    #print(\"The {} with ID {} should be linked with the following {}:\\n{}\".format(class_left, uri_idenifier_left, class_right, predictions[uri_idenifier_left]), end=\"\\n\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results in JSON to enable the definition of WeavingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_path = osp.join('../predicted/', DATASET_LABEL, model.get_name() + BATCH + \"_\" + directory_rule_name + \".json\")\n",
    "\n",
    "inv_right_mapping = {v: k for k, v in right_original_mapping.items()}\n",
    "json_dict = {}\n",
    "#iterate over the predictions and create the JSON\n",
    "for uri_idenifier_left, potential_links in predictions.items():\n",
    "    json_dict[uri_idenifier_left] = [inv_right_mapping[x] for x in potential_links]\n",
    "\n",
    "with open(predicted_path, 'w+') as f:\n",
    "    json.dump(json_dict, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save metrics and Hyperparameters in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = not osp.exists(save_path_csv)\n",
    "\n",
    "if new_file:\n",
    "    open(save_path_csv, 'w+').write(f\"'Best', 'Rule', 'Desc', 'ML_Arch_Name', 'encode_features_A', 'encode_features_B', 'Training_split', 'Epochs', 'Training_negative_edges', 'Learning Rate', 'Final validation score (ROC_AUC)'\\n\")\n",
    "\n",
    "\n",
    "with open(save_path_csv, 'a') as csvfile:\n",
    "\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    # result line\n",
    "    filewriter.writerow([\"\", directory_rule_name, folders[directory_rule_name], model.get_name(), \",\".join(['a']), \",\".join([\"b\"]), SPLIT, EPOCHS, str(ADD_NEGATIVE_TRAINING), LEARNING_RATE, '{0:.{1}f}'.format(val_roc_auc, 4)])\n",
    "\n",
    "\n",
    "trained_path = osp.join(trained_path, model.get_name() + BATCH + \"_\" + directory_rule_name + \".pth\")\n",
    "torch.save(model.state_dict(), trained_path)\n",
    "trained_path = osp.join('../trained/', DATASET_LABEL) # reset the name of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
